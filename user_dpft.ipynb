{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06542686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/2/bi000050/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train cols: ['user_id', 'input_ids', 'attention_mask', 'labels']\n",
      "Val cols: ['user_id', 'input_ids', 'attention_mask', 'labels']\n",
      "Train N: 4000 Eval N: 800 Users: 778\n",
      "\n",
      "=== Running user:dpsgd ===\n",
      "[Model] LoRA disabled. Training full model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/2/bi000050/courses/stat8931/dp/wandb/run-20251201_134403-469dctdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/469dctdc' target=\"_blank\">user-dpsgd-1764618243</a></strong> to <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/469dctdc' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/469dctdc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774a440978ab4ef5a402a4a2a346961a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "user:dpsgd | epoch 1/1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user:dpsgd] epoch 1: eval_loss=13.3493 ppl=627405.58  epsilon=1.227\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dp/clip_frac_users</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>dp/sample_rate_users</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>metrics/epsilon</td><td>▁</td></tr><tr><td>metrics/loss_epoch</td><td>▁</td></tr><tr><td>metrics/ppl</td><td>▁</td></tr><tr><td>train/iter</td><td>▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇█████</td></tr><tr><td>train/loss_iter</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dp/clip_frac_users</td><td>1</td></tr><tr><td>dp/sample_rate_users</td><td>0.00514</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>metrics/epsilon</td><td>1.22679</td></tr><tr><td>metrics/loss_epoch</td><td>13.34935</td></tr><tr><td>metrics/ppl</td><td>627405.58314</td></tr><tr><td>train/iter</td><td>1000</td></tr><tr><td>train/loss_iter</td><td>11.02115</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">user-dpsgd-1764618243</strong> at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/469dctdc' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/469dctdc</a><br> View project at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_134403-469dctdc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running user:dpdisfom ===\n",
      "[Model] LoRA disabled. Training full model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/2/bi000050/courses/stat8931/dp/wandb/run-20251201_135145-5crq72ku</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/5crq72ku' target=\"_blank\">user-dpdisfom-1764618705</a></strong> to <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/5crq72ku' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/5crq72ku</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011d22d193524dbd892ce77be23a366a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "user:dpdisfom | epoch 1/1:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user:dpdisfom] epoch 1: eval_loss=2.9055 ppl=18.27  epsilon=1.227\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dp/clip_frac_users</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>dp/sample_rate_users</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>metrics/epsilon</td><td>▁</td></tr><tr><td>metrics/loss_epoch</td><td>▁</td></tr><tr><td>metrics/ppl</td><td>▁</td></tr><tr><td>train/iter</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/loss_iter</td><td>▅▅▂▄▄▂▁▁▄▃▃▂▃▃▃▂▂▂▄▄▄▆▄▂▅▃▃▅▅▂▁▃▂▃▂▁▅▃▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dp/clip_frac_users</td><td>1</td></tr><tr><td>dp/sample_rate_users</td><td>0.00514</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>metrics/epsilon</td><td>1.22665</td></tr><tr><td>metrics/loss_epoch</td><td>2.90545</td></tr><tr><td>metrics/ppl</td><td>18.27348</td></tr><tr><td>train/iter</td><td>1000</td></tr><tr><td>train/loss_iter</td><td>3.28609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">user-dpdisfom-1764618705</strong> at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/5crq72ku' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/5crq72ku</a><br> View project at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_135145-5crq72ku/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dp_llm_compare_userlvl.csv\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers datasets peft opacus wandb\n",
    "\n",
    "import os, math, time, random, csv, gc\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Iterable, Optional, Dict\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from opacus.accountants.rdp import RDPAccountant\n",
    "\n",
    "# ----------------- W&B -----------------\n",
    "os.environ[\"WANDB_ENTITY\"]  = \"bi000050-university-of-minnesota\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"userdp_ft\"\n",
    "os.environ[\"WANDB_MODE\"]    = \"online\"\n",
    "wandb.login(key=\"0d32ee09cbe7bae59ddac577f978a4f31cc4b559\")\n",
    "USE_WANDB = True\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ----------------- Config -----------------\n",
    "@dataclass\n",
    "class CFG:\n",
    "    base_model: str = \"gpt2\"\n",
    "    use_8bit: bool = False\n",
    "\n",
    "    # LoRA\n",
    "    use_lora: bool = False\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_targets: Tuple[str, ...] = (\"c_attn\", \"c_fc\", \"c_proj\")\n",
    "\n",
    "    # Data\n",
    "    dataset_name: str = \"yahma/alpaca-cleaned\"\n",
    "    max_train_samples: int = 4000\n",
    "    max_eval_samples: int = 800\n",
    "    max_seq_len: int = 256\n",
    "\n",
    "    # DP level\n",
    "    privacy_level: str = \"user\"   # \"user\" or \"record\"\n",
    "    user_lam: float = 5.0         # synthetic users: Poisson(lam) items/user\n",
    "\n",
    "    # Train\n",
    "    epochs: int = 1\n",
    "    micro_batch: int = 4\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "    amp: bool = True              # autocast\n",
    "\n",
    "    # DP hyperparams\n",
    "    clip_norm: float = 10\n",
    "    noise_multiplier: float = 1\n",
    "    delta: float = 1e-5\n",
    "\n",
    "    # Methods to compare\n",
    "    methods: Tuple[str, ...] = (\"dpsgd\", \"dpdisfom\")\n",
    "\n",
    "    # DISFOM\n",
    "    rho_hat: float = 1\n",
    "    box_lo: Optional[float] = None\n",
    "    box_hi: Optional[float] = None\n",
    "\n",
    "    # Logging\n",
    "    log_csv: str = \"dp_llm_compare_userlvl.csv\"\n",
    "    save_root: str = \"dp_llm_runs\"\n",
    "\n",
    "CFG = CFG()\n",
    "Path(CFG.save_root).mkdir(exist_ok=True)\n",
    "\n",
    "# ----------------- Data (format → user_id → tokenize) -----------------\n",
    "def format_alpaca(e):\n",
    "    instr = e.get(\"instruction\",\"\"); inp = e.get(\"input\",\"\"); out = e.get(\"output\",\"\")\n",
    "    if inp:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n\"\n",
    "    return {\"text\": prompt + out}\n",
    "\n",
    "def build_raw_dataset():\n",
    "    raw = load_dataset(CFG.dataset_name)\n",
    "    raw = raw.map(format_alpaca, remove_columns=raw[\"train\"].column_names)\n",
    "    def take_first(ds, n): return ds.select(range(min(n, len(ds))))\n",
    "    train_raw = take_first(raw[\"train\"], CFG.max_train_samples)\n",
    "    eval_raw  = take_first(raw[\"train\"].select(range(CFG.max_train_samples, len(raw[\"train\"]))), CFG.max_eval_samples)\n",
    "    return DatasetDict({\"train\": train_raw, \"validation\": eval_raw})\n",
    "\n",
    "def attach_synthetic_user_ids(ds, lam=5.0, seed=1234):\n",
    "    import numpy as np\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(ds)\n",
    "    user_ids, uid, i = [], 0, 0\n",
    "    while i < n:\n",
    "        k = max(1, int(rng.poisson(lam)))\n",
    "        for _ in range(k):\n",
    "            if i >= n: break\n",
    "            user_ids.append(uid); i += 1\n",
    "        uid += 1\n",
    "    return ds.add_column(\"user_id\", user_ids[:n]), uid  # uid = #users\n",
    "\n",
    "raw_ds = build_raw_dataset()\n",
    "train_with_uid, U_total = attach_synthetic_user_ids(raw_ds[\"train\"], lam=CFG.user_lam, seed=2025)\n",
    "val_with_uid, _        = attach_synthetic_user_ids(raw_ds[\"validation\"], lam=CFG.user_lam, seed=2026)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.base_model, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tok(batch):\n",
    "    x = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=CFG.max_seq_len)\n",
    "    x[\"labels\"] = x[\"input_ids\"].copy()\n",
    "    return x\n",
    "\n",
    "tok_train = train_with_uid.map(tok, batched=True, remove_columns=[\"text\"])\n",
    "tok_val   = val_with_uid.map(tok,   batched=True, remove_columns=[\"text\"])\n",
    "print(\"Train cols:\", tok_train.column_names)\n",
    "print(\"Val cols:\", tok_val.column_names)\n",
    "\n",
    "core_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "class KeepUserID:\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __call__(self, features):\n",
    "        user_ids = [f[\"user_id\"] for f in features]\n",
    "        core = [{k:v for k,v in f.items() if k in (\"input_ids\",\"attention_mask\",\"labels\")} for f in features]\n",
    "        batch = self.base(core)\n",
    "        batch[\"user_id\"] = torch.tensor(user_ids, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "collator = KeepUserID(core_collator)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tok_train, batch_size=CFG.micro_batch, shuffle=True, drop_last=True,\n",
    "    collate_fn=collator, pin_memory=(device==\"cuda\"), num_workers=2, persistent_workers=True\n",
    ")\n",
    "eval_loader  = DataLoader(\n",
    "    tok_val, batch_size=CFG.micro_batch, shuffle=False, drop_last=False,\n",
    "    collate_fn=collator, pin_memory=(device==\"cuda\"), num_workers=2, persistent_workers=True\n",
    ")\n",
    "print(\"Train N:\", len(train_loader.dataset), \"Eval N:\", len(eval_loader.dataset), \"Users:\", U_total)\n",
    "\n",
    "# ----------------- Model -----------------\n",
    "def build_model():\n",
    "    load_kwargs = {}\n",
    "    if CFG.use_8bit:\n",
    "        load_kwargs.update(dict(load_in_8bit=True, device_map=\"auto\"))\n",
    "    model = AutoModelForCausalLM.from_pretrained(CFG.base_model, **load_kwargs)\n",
    "\n",
    "    if CFG.use_lora:\n",
    "        lcfg = LoraConfig(\n",
    "            r=CFG.lora_r, lora_alpha=CFG.lora_alpha, lora_dropout=CFG.lora_dropout,\n",
    "            target_modules=list(CFG.lora_targets), bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lcfg)\n",
    "        print(\"[Model] LoRA enabled.\")\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "        print(\"[Model] LoRA disabled. Training full model.\")\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.gradient_checkpointing_enable()\n",
    "    # ensure checkpointing has a grad-capturing input\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        model.get_input_embeddings().weight.requires_grad_(True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    return model\n",
    "\n",
    "def params_that_train(model):\n",
    "    return [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "def pack_params(params: Iterable[torch.Tensor]) -> Tuple[torch.Tensor, List[Tuple[int,...]]]:\n",
    "    flats, shapes = [], []\n",
    "    for p in params:\n",
    "        shapes.append(tuple(p.shape))\n",
    "        flats.append(p.detach().reshape(-1))\n",
    "    return torch.cat(flats), shapes\n",
    "\n",
    "def unpack_to_params(vec: torch.Tensor, params: Iterable[torch.Tensor], shapes: List[Tuple[int,...]]):\n",
    "    offset = 0\n",
    "    for p, shp in zip(params, shapes):\n",
    "        n = int(torch.tensor(shp).prod().item())\n",
    "        with torch.no_grad():\n",
    "            p.copy_(vec[offset:offset+n].view(shp))\n",
    "        offset += n\n",
    "\n",
    "# ----------------- Loss utils (AMP new API) -----------------\n",
    "AMP = CFG.amp and (device == \"cuda\")\n",
    "\n",
    "def per_example_losses_from_tensors(model, input_ids, labels, attention_mask, amp_enabled: bool):\n",
    "    amp_ctx = torch.amp.autocast(\"cuda\", enabled=amp_enabled) if (torch.cuda.is_available() and amp_enabled) else nullcontext()\n",
    "    with amp_ctx:\n",
    "        out = model(input_ids=input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        logits = out.logits\n",
    "        shift_logits = logits[:, :-1].contiguous()\n",
    "        shift_labels = labels[:, :-1].contiguous()\n",
    "        shift_mask   = attention_mask[:, :-1].contiguous().float()\n",
    "        logp = torch.nn.functional.log_softmax(shift_logits, dim=-1)\n",
    "        nll = torch.nn.functional.nll_loss(\n",
    "            logp.view(-1, logp.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            reduction=\"none\"\n",
    "        ).view(shift_labels.shape)\n",
    "        nll = nll * shift_mask\n",
    "        denom = shift_mask.sum(dim=1).clamp_min(1.0)\n",
    "        per_ex = nll.sum(dim=1) / denom      # [b]\n",
    "    if not per_ex.requires_grad:\n",
    "        raise RuntimeError(\"Chunk losses have no grad; ensure model.train() and no no_grad around forward.\")\n",
    "    return per_ex\n",
    "\n",
    "def evaluate_ppl(model, loader):\n",
    "    model.eval()\n",
    "    total, count = 0.0, 0\n",
    "    amp_ctx = torch.amp.autocast(\"cuda\", enabled=AMP) if device==\"cuda\" else nullcontext()\n",
    "    with torch.no_grad(), amp_ctx:\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels    = batch[\"labels\"].to(device)\n",
    "            attention = batch[\"attention_mask\"].to(device)\n",
    "            out = model(input_ids=input_ids, labels=labels, attention_mask=attention)\n",
    "            total += float(out.loss.item()) * input_ids.size(0)\n",
    "            count += input_ids.size(0)\n",
    "    mean = total / max(1, count)\n",
    "    ppl = math.exp(mean) if mean < 50 else float(\"inf\")\n",
    "    model.train()\n",
    "    return mean, ppl\n",
    "\n",
    "# ----------------- DP oracles (chunked forward+backward) -----------------\n",
    "def dp_recordlevel_oracle(model, batch, clip_C: float, noise_sigma: float, chunk_size: int = 2):\n",
    "    params = params_that_train(model)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "    attn      = batch[\"attention_mask\"].to(device)\n",
    "    B = int(input_ids.size(0))\n",
    "    per_grads = []\n",
    "\n",
    "    for start in range(0, B, chunk_size):\n",
    "        end = min(start + chunk_size, B)\n",
    "        ids_sl  = input_ids[start:end]\n",
    "        labs_sl = labels[start:end]\n",
    "        attn_sl = attn[start:end]\n",
    "\n",
    "        # fresh graph per chunk\n",
    "        losses_sl = per_example_losses_from_tensors(model, ids_sl, labs_sl, attn_sl, AMP)\n",
    "        s = int(losses_sl.shape[0])\n",
    "\n",
    "        for j in range(s):\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            losses_sl[j].backward(retain_graph=(j < s-1))\n",
    "            g_j = [(p.grad.detach().clone() if p.grad is not None else torch.zeros_like(p)) for p in params]\n",
    "            per_grads.append(g_j)\n",
    "\n",
    "    clipped = []\n",
    "    for g in per_grads:\n",
    "        flat, _ = pack_params(g)\n",
    "        norm = flat.norm(2)\n",
    "        scale = min(1.0, clip_C / (norm + 1e-12))\n",
    "        clipped.append([gi * scale for gi in g])\n",
    "\n",
    "    avg = [sum(gi_list)/B for gi_list in zip(*clipped)]\n",
    "    std = noise_sigma * clip_C / B\n",
    "    noisy = [g + torch.randn_like(g) * std for g in avg]\n",
    "    return noisy, B\n",
    "\n",
    "def dp_userlevel_oracle(model, batch, clip_C_user: float, noise_sigma: float, chunk_size: int = 2):\n",
    "    params = params_that_train(model)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    labels    = batch[\"labels\"].to(device)\n",
    "    attn      = batch[\"attention_mask\"].to(device)\n",
    "    user_ids  = batch[\"user_id\"].to(device)\n",
    "    B = int(input_ids.size(0))\n",
    "\n",
    "    per_grads, per_users = [], []\n",
    "\n",
    "    for start in range(0, B, chunk_size):\n",
    "        end = min(start + chunk_size, B)\n",
    "        ids_sl  = input_ids[start:end]\n",
    "        labs_sl = labels[start:end]\n",
    "        attn_sl = attn[start:end]\n",
    "        uid_sl  = user_ids[start:end]\n",
    "\n",
    "        losses_sl = per_example_losses_from_tensors(model, ids_sl, labs_sl, attn_sl, AMP)\n",
    "        s = int(losses_sl.shape[0])\n",
    "\n",
    "        for j in range(s):\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            losses_sl[j].backward(retain_graph=(j < s-1))\n",
    "            g_j = [(p.grad.detach().clone() if p.grad is not None else torch.zeros_like(p)) for p in params]\n",
    "            per_grads.append(g_j)\n",
    "            per_users.append(int(uid_sl[j].item()))\n",
    "\n",
    "    uniq_users = sorted(set(per_users))\n",
    "    user_grad: Dict[int, List[torch.Tensor]] = {u: [torch.zeros_like(p) for p in params] for u in uniq_users}\n",
    "    for gi, u in zip(per_grads, per_users):\n",
    "        for j, gij in enumerate(gi):\n",
    "            user_grad[u][j] += gij\n",
    "\n",
    "    clipped_users, clip_hits = [], 0\n",
    "    for u in uniq_users:\n",
    "        gu = user_grad[u]\n",
    "        flat, _ = pack_params(gu)\n",
    "        norm = flat.norm(2)\n",
    "        scale = min(1.0, clip_C_user / (norm + 1e-12))\n",
    "        if scale < 1.0: clip_hits += 1\n",
    "        clipped_users.append([gij * scale for gij in gu])\n",
    "\n",
    "    U_B = len(uniq_users)\n",
    "    avg = [sum(gs[j] for gs in clipped_users) / U_B for j in range(len(params))]\n",
    "    std = noise_sigma * clip_C_user / U_B\n",
    "    noisy = [g + torch.randn_like(g) * std for g in avg]\n",
    "    return noisy, U_B, clip_hits / max(1, U_B)\n",
    "\n",
    "# ----------------- Optimizer steps -----------------\n",
    "def apply_weight_decay(params, wd):\n",
    "    if wd == 0: return\n",
    "    with torch.no_grad():\n",
    "        for p in params: p.add_(-wd * p)\n",
    "\n",
    "def step_dpsgd(model, lr, private_grad):\n",
    "    with torch.no_grad():\n",
    "        for p, g in zip(params_that_train(model), private_grad):\n",
    "            p.add_(-lr * g)\n",
    "\n",
    "def step_dpnsgd(model, lr, private_grad, eps=1e-12):\n",
    "    gvec = torch.cat([g.view(-1) for g in private_grad])\n",
    "    norm = gvec.norm(2).clamp_min(eps)\n",
    "    scale = 1.0 / norm\n",
    "    with torch.no_grad():\n",
    "        for p, g in zip(params_that_train(model), private_grad):\n",
    "            p.add_(-lr * (g * scale))\n",
    "\n",
    "# DISFOM prox with φ(z)=(ρ̂/2)||z||_1^2\n",
    "def tau_by_bisection(u: torch.Tensor, rho_hat: float, max_iter: int = 80, tol: float = 1e-10) -> float:\n",
    "    lo, hi = 0.0, float(u.abs().max().item())\n",
    "    def rhs(tau: float) -> float:\n",
    "        return float(torch.nn.functional.relu(u.abs() - tau).sum().item())\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5*(lo+hi)\n",
    "        Rmid = mid - rho_hat * rhs(mid)\n",
    "        if abs(Rmid) <= tol: return mid\n",
    "        if Rmid > 0: hi = mid\n",
    "        else:        lo = mid\n",
    "    return 0.5*(lo+hi)\n",
    "\n",
    "def prox_disfom_unconstrained(u: torch.Tensor, rho_hat: float) -> torch.Tensor:\n",
    "    tau = tau_by_bisection(u, rho_hat)\n",
    "    return torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "\n",
    "def step_dpdisfom(model, lr, private_grad, rho_hat, box_lo=None, box_hi=None):\n",
    "    params = params_that_train(model)\n",
    "    with torch.no_grad():\n",
    "        xk_vec, shapes = pack_params([p.data for p in params])\n",
    "        g_vec, _ = pack_params(private_grad)\n",
    "        u = (-lr) * g_vec\n",
    "        if box_lo is None and box_hi is None:\n",
    "            y = prox_disfom_unconstrained(u, rho_hat)\n",
    "            x_next = xk_vec + y\n",
    "        else:\n",
    "            l = torch.full_like(xk_vec, box_lo) if box_lo is not None else torch.full_like(xk_vec, -float(\"inf\"))\n",
    "            ubox = torch.full_like(xk_vec, box_hi) if box_hi is not None else torch.full_like(xk_vec,  float(\"inf\"))\n",
    "            l_disp, u_disp = l - xk_vec, ubox - xk_vec\n",
    "            def rhs_box(tau: float) -> float:\n",
    "                z = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "                z = torch.max(torch.min(z, u_disp), l_disp)\n",
    "                return float(z.abs().sum().item())\n",
    "            lo, hi = 0.0, float(u.abs().max().item())\n",
    "            for _ in range(80):\n",
    "                mid = 0.5*(lo+hi)\n",
    "                Rmid = mid - rho_hat * rhs_box(mid)\n",
    "                if abs(Rmid) <= 1e-10: \n",
    "                    tau = mid; break\n",
    "                if Rmid > 0: hi = mid\n",
    "                else:        lo = mid\n",
    "            else:\n",
    "                tau = 0.5*(lo+hi)\n",
    "            z = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "            y = torch.max(torch.min(z, u_disp), l_disp)\n",
    "            x_next = xk_vec + y\n",
    "        unpack_to_params(x_next, params, shapes)\n",
    "\n",
    "# ----------------- Train loop -----------------\n",
    "def run_one_method(method: str, train_loader: DataLoader, eval_loader: DataLoader, U_total_users: int):\n",
    "    model = build_model()\n",
    "    assert any(p.requires_grad for p in model.parameters()), \"No trainable parameters\"\n",
    "    params = params_that_train(model)\n",
    "    lr = CFG.lr\n",
    "\n",
    "    acct = RDPAccountant()\n",
    "\n",
    "    run = None\n",
    "    if USE_WANDB:\n",
    "        run = wandb.init(\n",
    "            project=os.environ[\"WANDB_PROJECT\"],\n",
    "            entity=os.environ.get(\"WANDB_ENTITY\"),\n",
    "            name=f\"{CFG.privacy_level}-{method}-{int(time.time())}\",\n",
    "            group=f\"{CFG.privacy_level}-optimizer-compare\",\n",
    "            config={\n",
    "                \"privacy_level\": CFG.privacy_level,\n",
    "                \"base_model\": CFG.base_model, \"method\": method, \"epochs\": CFG.epochs,\n",
    "                \"clip\": CFG.clip_norm, \"noise\": CFG.noise_multiplier, \"delta\": CFG.delta,\n",
    "                \"rho_hat\": CFG.rho_hat if method==\"dpdisfom\" else None,\n",
    "                \"lr\": lr, \"batch_size\": CFG.micro_batch, \"max_seq_len\": CFG.max_seq_len,\n",
    "                \"train_N\": len(train_loader.dataset), \"eval_N\": len(eval_loader.dataset),\n",
    "                \"U_total\": U_total_users\n",
    "            },\n",
    "            reinit=True,\n",
    "        )\n",
    "        wandb.define_metric(\"train/iter\")\n",
    "        wandb.define_metric(\"train/loss_iter\", step_metric=\"train/iter\")\n",
    "        wandb.define_metric(\"epoch\")\n",
    "        wandb.define_metric(\"metrics/loss_epoch\", step_metric=\"epoch\")\n",
    "        wandb.define_metric(\"metrics/epsilon\", step_metric=\"epoch\")\n",
    "\n",
    "    iter_losses, global_step = [], 0\n",
    "\n",
    "    for ep in range(CFG.epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"{CFG.privacy_level}:{method} | epoch {ep+1}/{CFG.epochs}\")\n",
    "        for batch in pbar:\n",
    "            for k in batch: batch[k] = batch[k].to(device)\n",
    "\n",
    "            if CFG.privacy_level == \"record\":\n",
    "                g_priv, B = dp_recordlevel_oracle(model, batch, CFG.clip_norm, CFG.noise_multiplier, chunk_size=2)\n",
    "                sample_rate = CFG.micro_batch / max(1, len(train_loader.dataset))\n",
    "                clip_frac_users = None\n",
    "            else:\n",
    "                g_priv, U_B, clip_frac_users = dp_userlevel_oracle(model, batch, CFG.clip_norm, CFG.noise_multiplier, chunk_size=2)\n",
    "                sample_rate = U_B / max(1, U_total_users)\n",
    "\n",
    "            amp_ctx = torch.amp.autocast(\"cuda\", enabled=AMP) if device==\"cuda\" else nullcontext()\n",
    "            with torch.no_grad(), amp_ctx:\n",
    "                out = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"], attention_mask=batch[\"attention_mask\"])\n",
    "                batch_loss = float(out.loss.detach().cpu().item())\n",
    "\n",
    "            apply_weight_decay(params, CFG.weight_decay)\n",
    "            if method == \"dpsgd\":\n",
    "                step_dpsgd(model, lr, g_priv)\n",
    "            elif method == \"dpdisfom\":\n",
    "                step_dpdisfom(model, lr, g_priv, CFG.rho_hat, CFG.box_lo, CFG.box_hi)\n",
    "            else:\n",
    "                raise ValueError(method)\n",
    "\n",
    "            acct.step(noise_multiplier=CFG.noise_multiplier, sample_rate=sample_rate)\n",
    "\n",
    "            for p in params: p.grad = None\n",
    "            del g_priv, out\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available() and global_step % 50 == 0:\n",
    "                torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
    "\n",
    "            iter_losses.append(batch_loss)\n",
    "            global_step += 1\n",
    "            log_dict = {\"train/iter\": global_step, \"train/loss_iter\": batch_loss}\n",
    "            if CFG.privacy_level == \"user\" and clip_frac_users is not None:\n",
    "                log_dict[\"dp/clip_frac_users\"] = clip_frac_users\n",
    "                log_dict[\"dp/sample_rate_users\"] = sample_rate\n",
    "            pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "            if run is not None:\n",
    "                wandb.log(log_dict, step=global_step)\n",
    "\n",
    "        eval_loss, eval_ppl = evaluate_ppl(model, eval_loader)\n",
    "        eps_now = acct.get_epsilon(delta=CFG.delta)\n",
    "        print(f\"[{CFG.privacy_level}:{method}] epoch {ep+1}: eval_loss={eval_loss:.4f} ppl={eval_ppl:.2f}  epsilon={eps_now:.3f}\")\n",
    "        if run is not None:\n",
    "            wandb.log({\"epoch\": ep+1,\n",
    "                       \"metrics/loss_epoch\": eval_loss,\n",
    "                       \"metrics/ppl\": eval_ppl,\n",
    "                       \"metrics/epsilon\": eps_now}, step=global_step)\n",
    "\n",
    "    tag = f\"{CFG.privacy_level}_{CFG.base_model.replace('/','_')}_{method}\"\n",
    "    outdir = Path(CFG.save_root) / tag\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(outdir.as_posix())\n",
    "    if run is not None: run.finish()\n",
    "    return iter_losses\n",
    "\n",
    "# ----------------- Run all methods -----------------\n",
    "log_path = Path(CFG.log_csv)\n",
    "new_file = not log_path.exists()\n",
    "loss_traces = {}\n",
    "\n",
    "with open(log_path, \"a\", newline=\"\") as f:\n",
    "    wr = csv.writer(f)\n",
    "    if new_file:\n",
    "        wr.writerow([\"timestamp\",\"privacy_level\",\"base_model\",\"method\",\"epochs\",\"clip\",\"noise\",\"delta\",\n",
    "                     \"train_N\",\"eval_N\",\"U_total\",\"rho_hat\",\"lr\",\"last_eval_loss\",\"last_eval_ppl\",\"last_epsilon\"])\n",
    "\n",
    "    for m in CFG.methods:\n",
    "        print(f\"\\n=== Running {CFG.privacy_level}:{m} ===\")\n",
    "        iter_losses = run_one_method(m, train_loader, eval_loader, U_total_users=U_total)\n",
    "        loss_traces[m] = iter_losses\n",
    "        wr.writerow([int(time.time()), CFG.privacy_level, CFG.base_model, m, CFG.epochs,\n",
    "                     CFG.clip_norm, CFG.noise_multiplier, CFG.delta,\n",
    "                     len(train_loader.dataset), len(eval_loader.dataset), U_total,\n",
    "                     CFG.rho_hat if m==\"dpdisfom\" else \"\",\n",
    "                     CFG.lr, None, None, None])\n",
    "\n",
    "print(f\"Wrote {CFG.log_csv}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "for m, losses in loss_traces.items():\n",
    "    plt.plot(range(1, len(losses)+1), losses, label=m)\n",
    "plt.xlabel(\"Iteration\"); plt.ylabel(\"Training loss (per-batch)\")\n",
    "plt.title(f\"Loss vs Iteration ({CFG.privacy_level}-level DP)\")\n",
    "plt.legend(); plt.savefig(f\"loss_{CFG.privacy_level}_dp.png\"); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63639100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7387ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbi000050\u001b[0m (\u001b[33mbi000050-university-of-minnesota\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/2/bi000050/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %% imports & setup\n",
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Iterable, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from opacus.accountants.rdp import RDPAccountant\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "import wandb\n",
    "\n",
    "# --- W&B env (set your key via env var for security) ---\n",
    "os.environ.setdefault(\"WANDB_ENTITY\",  \"bi000050-university-of-minnesota\")\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"userdp_ft\")\n",
    "os.environ.setdefault(\"WANDB_MODE\",    \"online\")  # or \"offline\"\n",
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\", \"0d32ee09cbe7bae59ddac577f978a4f31cc4b559\")\n",
    "wandb.login(key=wandb_api_key)\n",
    "USE_WANDB = True\n",
    "\n",
    "# (optional) reduce fragmentation\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af23fcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(scenario='matched_epsilon_record', base_model='gpt2', use_8bit=False, use_lora=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_targets=('c_attn', 'c_fc', 'c_proj'), dataset_name='yahma/alpaca-cleaned', max_train_samples=2000, max_eval_samples=500, max_seq_len=256, epochs=10, micro_batch=4, lr_sgd=0.0001, lr_disfom=0.0001, weight_decay=0.0, clip_norm=5, noise_multiplier=3, delta=1e-05, target_epsilon=1, rho_hat=1, box_lo=None, box_hi=None, save_root='dp_llm_runs', log_csv='dp_llm_compare.csv')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% config (OOM-safe defaults; you can scale up later)\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # choose scenario: \"matched_epsilon_record\" | \"matched_epsilon_user\" | \"noise_stress\"\n",
    "    scenario: str = \"matched_epsilon_record\"\n",
    "\n",
    "    # model / LoRA\n",
    "    base_model: str = \"gpt2\"   # switch to \"gpt2\" after validating\n",
    "    use_8bit: bool = False\n",
    "    use_lora: bool = False            # LoRA drastically cuts memory\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_targets: Tuple[str, ...] = (\"c_attn\", \"c_fc\", \"c_proj\")\n",
    "\n",
    "    # data\n",
    "    dataset_name: str = \"yahma/alpaca-cleaned\"\n",
    "    max_train_samples: int = 2000\n",
    "    max_eval_samples: int = 500\n",
    "    max_seq_len: int = 256          # reduce if memory tight (e.g., 192 or 128)\n",
    "\n",
    "    # training\n",
    "    epochs: int = 10\n",
    "    micro_batch: int = 4            # small to allow per-example grads\n",
    "    lr_sgd: float = 1e-4\n",
    "    lr_disfom: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "    # DP privacy knobs\n",
    "    clip_norm: float = 5\n",
    "    noise_multiplier: float = 3   # used in noise_stress scenario\n",
    "    delta: float = 1e-5\n",
    "    target_epsilon: float = 1     # used in matched_epsilon_* scenarios\n",
    "\n",
    "    # DISFOM\n",
    "    rho_hat: float = 1\n",
    "    box_lo: Optional[float] = None\n",
    "    box_hi: Optional[float] = None\n",
    "\n",
    "    # noise_stress sweep\n",
    "    # stress_sigmas: Tuple[float, ...] = (0.6, 0.8, 1.0, 1.2)\n",
    "\n",
    "    # IO\n",
    "    save_root: str = \"dp_llm_runs\"\n",
    "    log_csv: str = \"dp_llm_compare.csv\"\n",
    "\n",
    "CFG = CFG()\n",
    "Path(CFG.save_root).mkdir(exist_ok=True)\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae97836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f0088e08fc48659a58c9f02bf66b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36161bd4a574d0fa97aa9baae6a1469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train N: 2000 Eval N: 500 Users: None\n"
     ]
    }
   ],
   "source": [
    "# %% dataset, tokenization, optional synthetic user IDs (for user-level DP)\n",
    "def format_alpaca(e):\n",
    "    instr = e.get(\"instruction\",\"\"); inp = e.get(\"input\",\"\"); out = e.get(\"output\",\"\")\n",
    "    if inp:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n\"\n",
    "    return {\"text\": prompt + out}\n",
    "\n",
    "def build_dataset_and_loaders(add_user_ids: bool = False):\n",
    "    raw = load_dataset(CFG.dataset_name)\n",
    "    raw = raw.map(format_alpaca, remove_columns=raw[\"train\"].column_names)\n",
    "\n",
    "    def take_first(ds, n): return ds.select(range(min(n, len(ds))))\n",
    "    train_raw = take_first(raw[\"train\"], CFG.max_train_samples)\n",
    "    eval_raw  = take_first(raw[\"train\"].select(range(CFG.max_train_samples, len(raw[\"train\"]))), CFG.max_eval_samples)\n",
    "    dataset = DatasetDict({\"train\": train_raw, \"validation\": eval_raw})\n",
    "\n",
    "    total_users = None\n",
    "    if add_user_ids:\n",
    "        def add_synth_user(ds, lam=5, seed=42):\n",
    "            rng = np.random.default_rng(seed)\n",
    "            n = len(ds); user_ids=[]; uid=0; i=0\n",
    "            while i<n:\n",
    "                k = max(1, int(rng.poisson(lam)))\n",
    "                for _ in range(k):\n",
    "                    if i>=n: break\n",
    "                    user_ids.append(uid); i+=1\n",
    "                uid+=1\n",
    "            return ds.add_column(\"user_id\", user_ids[:n]), uid\n",
    "        dataset[\"train\"], total_users = add_synth_user(dataset[\"train\"], lam=5, seed=42)\n",
    "        dataset[\"validation\"], _ = add_synth_user(dataset[\"validation\"], lam=5, seed=43)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(CFG.base_model, use_fast=True)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "\n",
    "    def tok_map(batch):\n",
    "        x = tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=CFG.max_seq_len)\n",
    "        x[\"labels\"] = x[\"input_ids\"].copy()\n",
    "        if \"user_id\" in batch:\n",
    "            x[\"user_id\"] = batch[\"user_id\"]\n",
    "        return x\n",
    "\n",
    "    tokd = dataset.map(tok_map, batched=True)\n",
    "\n",
    "    base_collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "    class KeepUserID:\n",
    "        def __init__(self, base): self.base = base\n",
    "        def __call__(self, features):\n",
    "            keep_uid = \"user_id\" in features[0]\n",
    "            uids = [f[\"user_id\"] for f in features] if keep_uid else None\n",
    "            feats = [{k:v for k,v in f.items() if k not in (\"text\",\"user_id\")} for f in features]\n",
    "            batch = self.base(feats)\n",
    "            if keep_uid:\n",
    "                batch[\"user_id\"] = torch.tensor(uids, dtype=torch.long)\n",
    "            return batch\n",
    "\n",
    "    collator = KeepUserID(base_collator)\n",
    "\n",
    "    train_loader = DataLoader(tokd[\"train\"], batch_size=CFG.micro_batch,\n",
    "                              shuffle=True, drop_last=True, collate_fn=collator)\n",
    "    eval_loader  = DataLoader(tokd[\"validation\"], batch_size=CFG.micro_batch,\n",
    "                              shuffle=False, drop_last=False, collate_fn=collator)\n",
    "    return dataset, train_loader, eval_loader, tok, total_users\n",
    "\n",
    "need_user = (CFG.scenario == \"matched_epsilon_user\")\n",
    "dataset, train_loader, eval_loader, tokenizer, TOTAL_USERS = build_dataset_and_loaders(add_user_ids=need_user)\n",
    "print(\"Train N:\", len(train_loader.dataset), \"Eval N:\", len(eval_loader.dataset), \"Users:\", TOTAL_USERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61ba3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% model builder (LoRA on/off + optional gradient checkpointing)\n",
    "def build_model():\n",
    "    load_kwargs = {}\n",
    "    if CFG.use_8bit:\n",
    "        load_kwargs.update(dict(load_in_8bit=True, device_map=\"auto\"))\n",
    "    model = AutoModelForCausalLM.from_pretrained(CFG.base_model, **load_kwargs)\n",
    "    if CFG.use_lora:\n",
    "        lcfg = LoraConfig(r=CFG.lora_r, lora_alpha=CFG.lora_alpha, lora_dropout=CFG.lora_dropout,\n",
    "                          target_modules=list(CFG.lora_targets), bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "        model = get_peft_model(model, lcfg)\n",
    "        print(\"[Model] LoRA enabled.\")\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "        print(\"[Model] LoRA disabled (full fine-tune).\")\n",
    "\n",
    "    # (optional) gradient checkpointing for larger models\n",
    "    try:\n",
    "        model.config.use_cache = False\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"[Model] Gradient checkpointing enabled.\")\n",
    "    except Exception as e:\n",
    "        print(\"[Model] Gradient checkpointing not available:\", e)\n",
    "\n",
    "    model.to(device).train()\n",
    "    return model\n",
    "\n",
    "def params_that_train(model):\n",
    "    return [p for p in model.parameters() if p.requires_grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d673bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% low-memory helpers: no GPU cat, CPU clip/avg/noise\n",
    "def zero_like_params(params: Iterable[torch.Tensor], device=\"cpu\"):\n",
    "    return [torch.zeros_like(p, device=device, dtype=p.dtype) for p in params]\n",
    "\n",
    "def l2norm_list(params_list: List[torch.Tensor]) -> float:\n",
    "    s = 0.0\n",
    "    for t in params_list:\n",
    "        s += float(t.float().pow(2).sum().item())\n",
    "    return s ** 0.5\n",
    "\n",
    "def scale_inplace(lst: List[torch.Tensor], scale: float):\n",
    "    for i in range(len(lst)):\n",
    "        lst[i].mul_(scale)\n",
    "\n",
    "def add_inplace(dst: List[torch.Tensor], src: List[torch.Tensor], alpha: float = 1.0):\n",
    "    for i in range(len(dst)):\n",
    "        dst[i].add_(src[i], alpha=alpha)\n",
    "\n",
    "def clone_to_cpu(lst: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    return [t.detach().to(\"cpu\", non_blocking=True).clone() for t in lst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d41fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% DP gradient oracles (record-level & user-level) -- MEMORY SAFE\n",
    "def dp_gradient_oracle_record(model, batch, clip_C: float, noise_sigma: float) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Per-example grads via re-forward (no retain_graph), clip/avg/noise on CPU.\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    params = params_that_train(model)\n",
    "\n",
    "    ids  = batch[\"input_ids\"]\n",
    "    labs = batch[\"labels\"]\n",
    "    amsk = batch[\"attention_mask\"]\n",
    "    B = ids.size(0)\n",
    "\n",
    "    sum_grad_cpu = zero_like_params(params, device=\"cpu\")\n",
    "\n",
    "    for i in range(B):\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out = model(\n",
    "            input_ids=ids[i:i+1].to(model_device, non_blocking=True),\n",
    "            labels=labs[i:i+1].to(model_device, non_blocking=True),\n",
    "            attention_mask=amsk[i:i+1].to(model_device, non_blocking=True),\n",
    "        )\n",
    "        out.loss.backward()\n",
    "        g_i_gpu = [p.grad if p.grad is not None else torch.zeros_like(p) for p in params]\n",
    "        g_i = clone_to_cpu(g_i_gpu)\n",
    "\n",
    "        norm = l2norm_list(g_i)\n",
    "        scale = min(1.0, clip_C / (norm + 1e-12))\n",
    "        scale_inplace(g_i, scale)\n",
    "\n",
    "        add_inplace(sum_grad_cpu, g_i, alpha=1.0 / B)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    std = noise_sigma * clip_C / B\n",
    "    noisy_avg_cpu = [g + torch.randn_like(g, device=\"cpu\") * std for g in sum_grad_cpu]\n",
    "    private_grad = [g.to(model_device, non_blocking=True) for g in noisy_avg_cpu]\n",
    "    return private_grad\n",
    "\n",
    "def dp_gradient_oracle_user(model, batch, clip_C_user: float, noise_sigma: float):\n",
    "    \"\"\"\n",
    "    User-level DP: sum by user on CPU, clip per-user, average users, add noise (CPU).\n",
    "    Returns (private_grad_on_device, U_B).\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    params = params_that_train(model)\n",
    "\n",
    "    ids  = batch[\"input_ids\"]\n",
    "    labs = batch[\"labels\"]\n",
    "    amsk = batch[\"attention_mask\"]\n",
    "    uids = batch[\"user_id\"]\n",
    "    B = ids.size(0)\n",
    "\n",
    "    uniq = torch.unique(uids).tolist()\n",
    "    user_sum = {int(u): zero_like_params(params, device=\"cpu\") for u in uniq}\n",
    "\n",
    "    for i in range(B):\n",
    "        u = int(uids[i].item())\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out = model(\n",
    "            input_ids=ids[i:i+1].to(model_device, non_blocking=True),\n",
    "            labels=labs[i:i+1].to(model_device, non_blocking=True),\n",
    "            attention_mask=amsk[i:i+1].to(model_device, non_blocking=True),\n",
    "        )\n",
    "        out.loss.backward()\n",
    "        g_i_gpu = [p.grad if p.grad is not None else torch.zeros_like(p) for p in params]\n",
    "        g_i = clone_to_cpu(g_i_gpu)\n",
    "        add_inplace(user_sum[u], g_i, alpha=1.0)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    clipped_users = []\n",
    "    for u in uniq:\n",
    "        g_u = user_sum[u]\n",
    "        nrm = l2norm_list(g_u)\n",
    "        scale = min(1.0, clip_C_user / (nrm + 1e-12))\n",
    "        scale_inplace(g_u, scale)\n",
    "        clipped_users.append(g_u)\n",
    "\n",
    "    U_B = len(uniq)\n",
    "    avg_cpu = zero_like_params(params, device=\"cpu\")\n",
    "    for g_u in clipped_users:\n",
    "        add_inplace(avg_cpu, g_u, alpha=1.0 / U_B)\n",
    "\n",
    "    std = noise_sigma * clip_C_user / U_B\n",
    "    noisy_avg_cpu = [g + torch.randn_like(g, device=\"cpu\") * std for g in avg_cpu]\n",
    "    private_grad = [g.to(model_device, non_blocking=True) for g in noisy_avg_cpu]\n",
    "    return private_grad, U_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbba79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% updates: weight decay, DP-SGD, DP-DISFOM (prox on displacement, CPU-safe)\n",
    "def apply_weight_decay(params, wd):\n",
    "    if wd == 0: return\n",
    "    with torch.no_grad():\n",
    "        for p in params: p.add_(-wd * p)\n",
    "\n",
    "def step_dpsgd(model, lr, private_grad):\n",
    "    with torch.no_grad():\n",
    "        for p, g in zip(params_that_train(model), private_grad):\n",
    "            p.add_(-lr * g)\n",
    "\n",
    "# DISFOM prox φ(z)=(ρ̂/2)||z||_1^2 applied to displacement u = -lr·ĝ\n",
    "def tau_by_bisection(u: torch.Tensor, rho_hat: float, max_iter: int = 80, tol: float = 1e-12) -> float:\n",
    "    lo, hi = 0.0, float(u.abs().max().item())\n",
    "    def rhs(tau: float) -> float:\n",
    "        return float(torch.nn.functional.relu(u.abs() - tau).sum().item())\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5*(lo+hi)\n",
    "        R = mid - rho_hat * rhs(mid)\n",
    "        if abs(R) <= tol: return mid\n",
    "        if R > 0: hi = mid\n",
    "        else:     lo = mid\n",
    "    return 0.5*(lo+hi)\n",
    "\n",
    "def step_dpdisfom(model, lr, private_grad, rho_hat, box_lo=None, box_hi=None):\n",
    "    \"\"\"\n",
    "    CPU-friendly prox: flatten on CPU to avoid peak GPU memory.\n",
    "    \"\"\"\n",
    "    params = params_that_train(model)\n",
    "    dev = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        # move to CPU\n",
    "        xk_list = [p.data.detach().cpu().clone() for p in params]\n",
    "        g_list  = [g.detach().cpu().clone() for g in private_grad]\n",
    "\n",
    "        xk_vec = torch.cat([t.view(-1) for t in xk_list])\n",
    "        g_vec  = torch.cat([t.view(-1) for t in g_list])\n",
    "        u = (-lr) * g_vec\n",
    "\n",
    "        if box_lo is None and box_hi is None:\n",
    "            tau = tau_by_bisection(u, rho_hat)\n",
    "            y = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "            x_next = xk_vec + y\n",
    "        else:\n",
    "            l = torch.full_like(xk_vec, box_lo) if box_lo is not None else torch.full_like(xk_vec, -float(\"inf\"))\n",
    "            ubox = torch.full_like(xk_vec, box_hi) if box_hi is not None else torch.full_like(xk_vec,  float(\"inf\"))\n",
    "            l_disp, u_disp = l - xk_vec, ubox - xk_vec\n",
    "            def rhs_box(tau: float) -> float:\n",
    "                z = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "                z = torch.max(torch.min(z, u_disp), l_disp)\n",
    "                return float(z.abs().sum().item())\n",
    "            lo, hi = 0.0, float(u.abs().max().item())\n",
    "            for _ in range(80):\n",
    "                mid = 0.5*(lo+hi)\n",
    "                R = mid - rho_hat * rhs_box(mid)\n",
    "                if abs(R) <= 1e-12: tau = mid; break\n",
    "                if R > 0: hi = mid\n",
    "                else:     lo = mid\n",
    "            else:\n",
    "                tau = 0.5*(lo+hi)\n",
    "            z = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "            y = torch.max(torch.min(z, u_disp), l_disp)\n",
    "            x_next = xk_vec + y\n",
    "\n",
    "        # write back to device params\n",
    "        offset = 0\n",
    "        for p in params:\n",
    "            n = p.numel()\n",
    "            chunk = x_next[offset:offset+n].view(p.shape).to(dev)\n",
    "            p.copy_(chunk)\n",
    "            offset += n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78f786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% eval: PPL + ROUGE/BLEU\n",
    "def eval_perplexity(model, eval_loader):\n",
    "    model.eval(); total, count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            for k in batch: batch[k] = batch[k].to(device)\n",
    "            out = model(**batch)\n",
    "            total += float(out.loss.item()) * batch[\"input_ids\"].size(0)\n",
    "            count += batch[\"input_ids\"].size(0)\n",
    "    mean = total / max(1, count)\n",
    "    ppl = math.exp(mean) if mean < 50 else float(\"inf\")\n",
    "    model.train()\n",
    "    return mean, ppl\n",
    "\n",
    "def split_prompt_and_ref(txt: str):\n",
    "    key = \"### Response:\\n\"\n",
    "    if key in txt:\n",
    "        i = txt.index(key)\n",
    "        return txt[:i+len(key)], txt[i+len(key):].strip()\n",
    "    return txt, \"\"\n",
    "\n",
    "def collect_eval_prompts_and_refs(dataset_validation, max_items=300):\n",
    "    prompts, refs = [], []\n",
    "    m = min(len(dataset_validation), max_items)\n",
    "    for i in range(m):\n",
    "        txt = dataset_validation[i][\"text\"]\n",
    "        p, r = split_prompt_and_ref(txt)\n",
    "        prompts.append(p); refs.append(r)\n",
    "    return prompts, refs\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_responses(model, tok, prompts, max_new_tokens=128, temperature=0.0, top_p=1.0):\n",
    "    outs=[]\n",
    "    for p in prompts:\n",
    "        inputs = tok(p, return_tensors=\"pt\").to(device)\n",
    "        ids = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                             do_sample=(temperature>0), temperature=temperature, top_p=top_p,\n",
    "                             pad_token_id=tok.eos_token_id)\n",
    "        full = tok.decode(ids[0], skip_special_tokens=True)\n",
    "        outs.append(full[len(p):].strip() if full.startswith(p) else full.strip())\n",
    "    return outs\n",
    "\n",
    "def eval_text_metrics(preds, refs):\n",
    "    r = rouge.compute(predictions=preds, references=refs)\n",
    "    b = bleu.compute(predictions=preds, references=[[x] for x in refs])\n",
    "    return {**r, \"bleu\": b[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a28bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% privacy helper: σ for target ε (record-style search)\n",
    "def sigma_for_epsilon_record(target_eps, delta, q, T, lo=0.3, hi=5.0):\n",
    "    for _ in range(25):\n",
    "        mid = 0.5*(lo+hi)\n",
    "        acct = RDPAccountant()\n",
    "        for _ in range(T): acct.step(noise_multiplier=mid, sample_rate=q)\n",
    "        eps = acct.get_epsilon(delta=delta)\n",
    "        if eps > target_eps: lo = mid\n",
    "        else:                hi = mid\n",
    "    return hi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25eb341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% training loop (scenario-aware)\n",
    "def train_once(method: str, sigma: float, train_loader, eval_loader,\n",
    "               privacy_mode: str, total_users: Optional[int] = None) -> Dict[str, float]:\n",
    "    assert privacy_mode in (\"record\",\"user\")\n",
    "    model = build_model()\n",
    "    params = params_that_train(model)\n",
    "    lr = CFG.lr_disfom if method == \"dpdisfom\" else CFG.lr_sgd\n",
    "\n",
    "    acct = RDPAccountant()\n",
    "\n",
    "    run = None\n",
    "    if USE_WANDB:\n",
    "        run = wandb.init(\n",
    "            project=os.environ[\"WANDB_PROJECT\"],\n",
    "            entity=os.environ.get(\"WANDB_ENTITY\"),\n",
    "            name=f\"{method}-{privacy_mode}-sigma{sigma}-{int(time.time())}\",\n",
    "            group=f\"{CFG.scenario}\",\n",
    "            config={\n",
    "                \"base_model\": CFG.base_model, \"method\": method,\n",
    "                \"epochs\": CFG.epochs, \"clip\": CFG.clip_norm,\n",
    "                \"sigma\": sigma, \"delta\": CFG.delta,\n",
    "                \"rho_hat\": CFG.rho_hat if method==\"dpdisfom\" else None,\n",
    "                \"lr\": lr, \"batch\": CFG.micro_batch, \"seq_len\": CFG.max_seq_len,\n",
    "                \"privacy_mode\": privacy_mode, \"use_lora\": CFG.use_lora\n",
    "            },\n",
    "            reinit=True,\n",
    "        )\n",
    "        wandb.define_metric(\"train/iter\")\n",
    "        wandb.define_metric(\"train/loss_iter\", step_metric=\"train/iter\")\n",
    "        wandb.define_metric(\"epoch\")\n",
    "        wandb.define_metric(\"metrics/epsilon\", step_metric=\"epoch\")\n",
    "        wandb.define_metric(\"metrics/loss_epoch\", step_metric=\"epoch\")\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(CFG.epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"{method} [{privacy_mode}] σ={sigma} | epoch {ep+1}/{CFG.epochs}\")\n",
    "        for batch in pbar:\n",
    "            for k in batch: batch[k] = batch[k].to(device)\n",
    "\n",
    "            if privacy_mode == \"record\":\n",
    "                g_priv = dp_gradient_oracle_record(model, batch, CFG.clip_norm, sigma)\n",
    "                sample_rate = CFG.micro_batch / len(train_loader.dataset)\n",
    "            else:\n",
    "                g_priv, U_B = dp_gradient_oracle_user(model, batch, CFG.clip_norm, sigma)\n",
    "                if total_users is None or total_users == 0:\n",
    "                    raise ValueError(\"total_users must be provided for user-level DP.\")\n",
    "                sample_rate = U_B / total_users\n",
    "\n",
    "            # pre-update loss for logging (approx same step)\n",
    "            with torch.no_grad():\n",
    "                out = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"], attention_mask=batch[\"attention_mask\"])\n",
    "                batch_loss = float(out.loss.detach().cpu().item())\n",
    "\n",
    "            apply_weight_decay(params, CFG.weight_decay)\n",
    "            if method == \"dpsgd\":\n",
    "                step_dpsgd(model, lr, g_priv)\n",
    "            elif method == \"dpdisfom\":\n",
    "                step_dpdisfom(model, lr, g_priv, CFG.rho_hat, CFG.box_lo, CFG.box_hi)\n",
    "            else:\n",
    "                raise ValueError(method)\n",
    "\n",
    "            acct.step(noise_multiplier=sigma, sample_rate=sample_rate)\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "            if run is not None:\n",
    "                wandb.log({\"train/iter\": global_step, \"train/loss_iter\": batch_loss}, step=global_step)\n",
    "\n",
    "        eval_loss, eval_ppl = eval_perplexity(model, eval_loader)\n",
    "        eps_now = acct.get_epsilon(delta=CFG.delta)\n",
    "        if run is not None:\n",
    "            wandb.log({\"epoch\": ep+1,\n",
    "                       \"metrics/loss_epoch\": eval_loss,\n",
    "                       \"metrics/ppl\": eval_ppl,\n",
    "                       \"metrics/epsilon\": eps_now}, step=global_step)\n",
    "\n",
    "    # save\n",
    "    tag = f\"{CFG.base_model.replace('/','_')}_{method}_{privacy_mode}_sigma{sigma}\"\n",
    "    outdir = Path(CFG.save_root)/tag\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(outdir.as_posix())\n",
    "\n",
    "    # final eval\n",
    "    prompts, refs = collect_eval_prompts_and_refs(dataset[\"validation\"], max_items=300)\n",
    "    model.eval()\n",
    "    preds = generate_responses(model, tokenizer, prompts, max_new_tokens=128, temperature=0.0)\n",
    "    text_metrics = eval_text_metrics(preds, refs)\n",
    "    ppl_loss, ppl = eval_perplexity(model, eval_loader)\n",
    "\n",
    "    if run is not None:\n",
    "        wandb.log({\n",
    "            \"final/rouge1\": text_metrics[\"rouge1\"],\n",
    "            \"final/rouge2\": text_metrics[\"rouge2\"],\n",
    "            \"final/rougeL\": text_metrics[\"rougeL\"],\n",
    "            \"final/bleu\":   text_metrics[\"bleu\"],\n",
    "            \"final/ppl_loss\": ppl_loss,\n",
    "            \"final/ppl\": ppl\n",
    "        })\n",
    "        run.finish()\n",
    "\n",
    "    return {\"model_dir\": outdir.as_posix(),\n",
    "            \"rougeL\": text_metrics[\"rougeL\"],\n",
    "            \"bleu\": text_metrics[\"bleu\"],\n",
    "            \"ppl\": ppl, \"ppl_loss\": ppl_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98e059f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% orchestrators\n",
    "def run_matched_epsilon_record():\n",
    "    print(\"== Matched-ε (Record-level DP) ==\")\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    T = CFG.epochs * steps_per_epoch\n",
    "    q = CFG.micro_batch / len(train_loader.dataset)\n",
    "    sigma = sigma_for_epsilon_record(CFG.target_epsilon, CFG.delta, q, T, lo=0.3, hi=5.0)\n",
    "    print(f\"ε_target={CFG.target_epsilon} -> σ≈{sigma:.3f} (q={q:.6f}, T={T})\")\n",
    "\n",
    "    res = {}\n",
    "    for method in (\"dpsgd\", \"dpdisfom\"):\n",
    "    # for method in (\"dpdisfom\",):\n",
    "        res[method] = train_once(method, sigma, train_loader, eval_loader, privacy_mode=\"record\")\n",
    "        print(method, res[method])\n",
    "    return res, sigma\n",
    "\n",
    "def run_matched_epsilon_user():\n",
    "    assert TOTAL_USERS is not None, \"Rebuild dataset with add_user_ids=True.\"\n",
    "    print(\"== Matched-ε (User-level DP) ==\")\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    T = CFG.epochs * steps_per_epoch\n",
    "    # expected distinct users per batch ~ micro_batch (Poisson(5) gives many-user batches); conservative:\n",
    "    q_user = min(1.0, CFG.micro_batch / max(1, TOTAL_USERS))\n",
    "    sigma = sigma_for_epsilon_record(CFG.target_epsilon, CFG.delta, q_user, T, lo=0.3, hi=5.0)\n",
    "    print(f\"ε_target={CFG.target_epsilon} -> σ≈{sigma:.3f} (q_user≈{q_user:.6f}, T={T}, users={TOTAL_USERS})\")\n",
    "\n",
    "    res = {}\n",
    "    for method in (\"dpsgd\", \"dpdisfom\"):\n",
    "    # for method in (\"dpdisfom\",):\n",
    "        res[method] = train_once(method, sigma, train_loader, eval_loader, privacy_mode=\"user\", total_users=TOTAL_USERS)\n",
    "        print(method, res[method])\n",
    "    return res, sigma\n",
    "\n",
    "def run_noise_stress():\n",
    "    print(\"== Noise-Stress (σ sweep) ==\")\n",
    "    table = []\n",
    "    for sigma in CFG.stress_sigmas:\n",
    "        for method in (\"dpsgd\", \"dpdisfom\"):\n",
    "            res = train_once(method, sigma, train_loader, eval_loader, privacy_mode=\"record\")\n",
    "            print(f\"sigma={sigma:.2f}\", method, res)\n",
    "            table.append({\"sigma\": sigma, \"method\": method, **res})\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14215d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Matched-ε (Record-level DP) ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε_target=1 -> σ≈1.005 (q=0.002000, T=5000)\n",
      "[Model] LoRA disabled (full fine-tune).\n",
      "[Model] Gradient checkpointing enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/2/bi000050/courses/stat8931/dp/wandb/run-20251202_154423-joyec8o7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/joyec8o7' target=\"_blank\">dpsgd-record-sigma1.00489399433136-1764711863</a></strong> to <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/joyec8o7' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/joyec8o7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191a5765e54f4a2e83cf0202fcf2287d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 1/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% execute chosen scenario\n",
    "if CFG.scenario == \"matched_epsilon_record\":\n",
    "    results, sigma_star = run_matched_epsilon_record()\n",
    "elif CFG.scenario == \"matched_epsilon_user\":\n",
    "    results, sigma_star = run_matched_epsilon_user()\n",
    "elif CFG.scenario == \"noise_stress\":\n",
    "    stress_table = run_noise_stress()\n",
    "else:\n",
    "    raise ValueError(\"Unknown CFG.scenario\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedfb11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
