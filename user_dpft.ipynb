{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7387ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbi000050\u001b[0m (\u001b[33mbi000050-university-of-minnesota\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/2/bi000050/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %% imports & setup\n",
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Iterable, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from opacus.accountants.rdp import RDPAccountant\n",
    "\n",
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "import wandb\n",
    "\n",
    "# --- W&B env (set your key via env var for security) ---\n",
    "os.environ.setdefault(\"WANDB_ENTITY\",  \"bi000050-university-of-minnesota\")\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"userdp_ft\")\n",
    "os.environ.setdefault(\"WANDB_MODE\",    \"online\")  # or \"offline\"\n",
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\", \"0d32ee09cbe7bae59ddac577f978a4f31cc4b559\")\n",
    "wandb.login(key=wandb_api_key)\n",
    "USE_WANDB = True\n",
    "\n",
    "# (optional) reduce fragmentation\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af23fcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(scenario='matched_epsilon_record', base_model='gpt2', use_8bit=False, use_lora=False, lora_r=16, lora_alpha=32, lora_dropout=0.05, lora_targets=('c_attn', 'c_fc', 'c_proj'), dataset_name='yahma/alpaca-cleaned', max_train_samples=2000, max_eval_samples=500, max_seq_len=256, epochs=10, micro_batch=4, lr_sgd=0.0001, lr_disfom=0.0001, weight_decay=0.0, clip_norm=5, noise_multiplier=3, delta=1e-05, target_epsilon=1, rho_hat=1, box_lo=None, box_hi=None, save_root='dp_llm_runs', log_csv='dp_llm_compare.csv')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% config (OOM-safe defaults; you can scale up later)\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # choose scenario: \"matched_epsilon_record\" | \"matched_epsilon_user\" | \"noise_stress\"\n",
    "    scenario: str = \"matched_epsilon_record\"\n",
    "\n",
    "    # model / LoRA\n",
    "    base_model: str = \"gpt2\"   # switch to \"gpt2\" after validating\n",
    "    use_8bit: bool = False\n",
    "    use_lora: bool = False            # LoRA drastically cuts memory\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_targets: Tuple[str, ...] = (\"c_attn\", \"c_fc\", \"c_proj\")\n",
    "\n",
    "    # data\n",
    "    dataset_name: str = \"yahma/alpaca-cleaned\"\n",
    "    max_train_samples: int = 2000\n",
    "    max_eval_samples: int = 500\n",
    "    max_seq_len: int = 256          # reduce if memory tight (e.g., 192 or 128)\n",
    "\n",
    "    # training\n",
    "    epochs: int = 10\n",
    "    micro_batch: int = 4            # small to allow per-example grads\n",
    "    lr_sgd: float = 1e-4\n",
    "    lr_disfom: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "    # DP privacy knobs\n",
    "    clip_norm: float = 5\n",
    "    noise_multiplier: float = 3   # used in noise_stress scenario\n",
    "    delta: float = 1e-5\n",
    "    target_epsilon: float = 1     # used in matched_epsilon_* scenarios\n",
    "\n",
    "    # DISFOM\n",
    "    rho_hat: float = 1\n",
    "    box_lo: Optional[float] = None\n",
    "    box_hi: Optional[float] = None\n",
    "\n",
    "    # noise_stress sweep\n",
    "    # stress_sigmas: Tuple[float, ...] = (0.6, 0.8, 1.0, 1.2)\n",
    "\n",
    "    # IO\n",
    "    save_root: str = \"dp_llm_runs\"\n",
    "    log_csv: str = \"dp_llm_compare.csv\"\n",
    "\n",
    "CFG = CFG()\n",
    "Path(CFG.save_root).mkdir(exist_ok=True)\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae97836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9049b79ecf774d1bba409e177648c902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train N: 2000 Eval N: 500 Users: None\n"
     ]
    }
   ],
   "source": [
    "# %% dataset, tokenization, optional synthetic user IDs (for user-level DP)\n",
    "def format_alpaca(e):\n",
    "    instr = e.get(\"instruction\",\"\"); inp = e.get(\"input\",\"\"); out = e.get(\"output\",\"\")\n",
    "    if inp:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instr}\\n\\n### Response:\\n\"\n",
    "    return {\"text\": prompt + out}\n",
    "\n",
    "def build_dataset_and_loaders(add_user_ids: bool = False):\n",
    "    raw = load_dataset(CFG.dataset_name)\n",
    "    raw = raw.map(format_alpaca, remove_columns=raw[\"train\"].column_names)\n",
    "\n",
    "    def take_first(ds, n): return ds.select(range(min(n, len(ds))))\n",
    "    train_raw = take_first(raw[\"train\"], CFG.max_train_samples)\n",
    "    eval_raw  = take_first(raw[\"train\"].select(range(CFG.max_train_samples, len(raw[\"train\"]))), CFG.max_eval_samples)\n",
    "    dataset = DatasetDict({\"train\": train_raw, \"validation\": eval_raw})\n",
    "\n",
    "    total_users = None\n",
    "    if add_user_ids:\n",
    "        def add_synth_user(ds, lam=5, seed=42):\n",
    "            rng = np.random.default_rng(seed)\n",
    "            n = len(ds); user_ids=[]; uid=0; i=0\n",
    "            while i<n:\n",
    "                k = max(1, int(rng.poisson(lam)))\n",
    "                for _ in range(k):\n",
    "                    if i>=n: break\n",
    "                    user_ids.append(uid); i+=1\n",
    "                uid+=1\n",
    "            return ds.add_column(\"user_id\", user_ids[:n]), uid\n",
    "        dataset[\"train\"], total_users = add_synth_user(dataset[\"train\"], lam=5, seed=42)\n",
    "        dataset[\"validation\"], _ = add_synth_user(dataset[\"validation\"], lam=5, seed=43)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(CFG.base_model, use_fast=True)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "\n",
    "    def tok_map(batch):\n",
    "        x = tok(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=CFG.max_seq_len)\n",
    "        x[\"labels\"] = x[\"input_ids\"].copy()\n",
    "        if \"user_id\" in batch:\n",
    "            x[\"user_id\"] = batch[\"user_id\"]\n",
    "        return x\n",
    "\n",
    "    tokd = dataset.map(tok_map, batched=True)\n",
    "\n",
    "    base_collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
    "\n",
    "    class KeepUserID:\n",
    "        def __init__(self, base): self.base = base\n",
    "        def __call__(self, features):\n",
    "            keep_uid = \"user_id\" in features[0]\n",
    "            uids = [f[\"user_id\"] for f in features] if keep_uid else None\n",
    "            feats = [{k:v for k,v in f.items() if k not in (\"text\",\"user_id\")} for f in features]\n",
    "            batch = self.base(feats)\n",
    "            if keep_uid:\n",
    "                batch[\"user_id\"] = torch.tensor(uids, dtype=torch.long)\n",
    "            return batch\n",
    "\n",
    "    collator = KeepUserID(base_collator)\n",
    "\n",
    "    train_loader = DataLoader(tokd[\"train\"], batch_size=CFG.micro_batch,\n",
    "                              shuffle=True, drop_last=True, collate_fn=collator)\n",
    "    eval_loader  = DataLoader(tokd[\"validation\"], batch_size=CFG.micro_batch,\n",
    "                              shuffle=False, drop_last=False, collate_fn=collator)\n",
    "    return dataset, train_loader, eval_loader, tok, total_users\n",
    "\n",
    "need_user = (CFG.scenario == \"matched_epsilon_user\")\n",
    "dataset, train_loader, eval_loader, tokenizer, TOTAL_USERS = build_dataset_and_loaders(add_user_ids=need_user)\n",
    "print(\"Train N:\", len(train_loader.dataset), \"Eval N:\", len(eval_loader.dataset), \"Users:\", TOTAL_USERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ba3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% model builder (LoRA on/off + optional gradient checkpointing)\n",
    "def build_model():\n",
    "    load_kwargs = {}\n",
    "    if CFG.use_8bit:\n",
    "        load_kwargs.update(dict(load_in_8bit=True, device_map=\"auto\"))\n",
    "    model = AutoModelForCausalLM.from_pretrained(CFG.base_model, **load_kwargs)\n",
    "    if CFG.use_lora:\n",
    "        lcfg = LoraConfig(r=CFG.lora_r, lora_alpha=CFG.lora_alpha, lora_dropout=CFG.lora_dropout,\n",
    "                          target_modules=list(CFG.lora_targets), bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "        model = get_peft_model(model, lcfg)\n",
    "        print(\"[Model] LoRA enabled.\")\n",
    "    else:\n",
    "        for p in model.parameters(): p.requires_grad = True\n",
    "        print(\"[Model] LoRA disabled (full fine-tune).\")\n",
    "\n",
    "    # (optional) gradient checkpointing for larger models\n",
    "    try:\n",
    "        model.config.use_cache = False\n",
    "        model.gradient_checkpointing_enable()\n",
    "        print(\"[Model] Gradient checkpointing enabled.\")\n",
    "    except Exception as e:\n",
    "        print(\"[Model] Gradient checkpointing not available:\", e)\n",
    "\n",
    "    model.to(device).train()\n",
    "    return model\n",
    "\n",
    "def params_that_train(model):\n",
    "    return [p for p in model.parameters() if p.requires_grad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d673bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% low-memory helpers: no GPU cat, CPU clip/avg/noise\n",
    "def zero_like_params(params: Iterable[torch.Tensor], device=\"cpu\"):\n",
    "    return [torch.zeros_like(p, device=device, dtype=p.dtype) for p in params]\n",
    "\n",
    "def l2norm_list(params_list: List[torch.Tensor]) -> float:\n",
    "    s = 0.0\n",
    "    for t in params_list:\n",
    "        s += float(t.float().pow(2).sum().item())\n",
    "    return s ** 0.5\n",
    "\n",
    "def scale_inplace(lst: List[torch.Tensor], scale: float):\n",
    "    for i in range(len(lst)):\n",
    "        lst[i].mul_(scale)\n",
    "\n",
    "def add_inplace(dst: List[torch.Tensor], src: List[torch.Tensor], alpha: float = 1.0):\n",
    "    for i in range(len(dst)):\n",
    "        dst[i].add_(src[i], alpha=alpha)\n",
    "\n",
    "def clone_to_cpu(lst: List[torch.Tensor]) -> List[torch.Tensor]:\n",
    "    return [t.detach().to(\"cpu\", non_blocking=True).clone() for t in lst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d41fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% DP gradient oracles (record-level & user-level) -- MEMORY SAFE\n",
    "def dp_gradient_oracle_record(model, batch, clip_C: float, noise_sigma: float) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Per-example grads via re-forward (no retain_graph), clip/avg/noise on CPU.\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    params = params_that_train(model)\n",
    "\n",
    "    ids  = batch[\"input_ids\"]\n",
    "    labs = batch[\"labels\"]\n",
    "    amsk = batch[\"attention_mask\"]\n",
    "    B = ids.size(0)\n",
    "\n",
    "    sum_grad_cpu = zero_like_params(params, device=\"cpu\")\n",
    "\n",
    "    for i in range(B):\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out = model(\n",
    "            input_ids=ids[i:i+1].to(model_device, non_blocking=True),\n",
    "            labels=labs[i:i+1].to(model_device, non_blocking=True),\n",
    "            attention_mask=amsk[i:i+1].to(model_device, non_blocking=True),\n",
    "        )\n",
    "        out.loss.backward()\n",
    "        g_i_gpu = [p.grad if p.grad is not None else torch.zeros_like(p) for p in params]\n",
    "        g_i = clone_to_cpu(g_i_gpu)\n",
    "\n",
    "        norm = l2norm_list(g_i)\n",
    "        scale = min(1.0, clip_C / (norm + 1e-12))\n",
    "        scale_inplace(g_i, scale)\n",
    "\n",
    "        add_inplace(sum_grad_cpu, g_i, alpha=1.0 / B)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    std = noise_sigma * clip_C / B\n",
    "    noisy_avg_cpu = [g + torch.randn_like(g, device=\"cpu\") * std for g in sum_grad_cpu]\n",
    "    private_grad = [g.to(model_device, non_blocking=True) for g in noisy_avg_cpu]\n",
    "    return private_grad\n",
    "\n",
    "def dp_gradient_oracle_user(model, batch, clip_C_user: float, noise_sigma: float):\n",
    "    \"\"\"\n",
    "    User-level DP: sum by user on CPU, clip per-user, average users, add noise (CPU).\n",
    "    Returns (private_grad_on_device, U_B).\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    params = params_that_train(model)\n",
    "\n",
    "    ids  = batch[\"input_ids\"]\n",
    "    labs = batch[\"labels\"]\n",
    "    amsk = batch[\"attention_mask\"]\n",
    "    uids = batch[\"user_id\"]\n",
    "    B = ids.size(0)\n",
    "\n",
    "    uniq = torch.unique(uids).tolist()\n",
    "    user_sum = {int(u): zero_like_params(params, device=\"cpu\") for u in uniq}\n",
    "\n",
    "    for i in range(B):\n",
    "        u = int(uids[i].item())\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        out = model(\n",
    "            input_ids=ids[i:i+1].to(model_device, non_blocking=True),\n",
    "            labels=labs[i:i+1].to(model_device, non_blocking=True),\n",
    "            attention_mask=amsk[i:i+1].to(model_device, non_blocking=True),\n",
    "        )\n",
    "        out.loss.backward()\n",
    "        g_i_gpu = [p.grad if p.grad is not None else torch.zeros_like(p) for p in params]\n",
    "        g_i = clone_to_cpu(g_i_gpu)\n",
    "        add_inplace(user_sum[u], g_i, alpha=1.0)\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "    clipped_users = []\n",
    "    for u in uniq:\n",
    "        g_u = user_sum[u]\n",
    "        nrm = l2norm_list(g_u)\n",
    "        scale = min(1.0, clip_C_user / (nrm + 1e-12))\n",
    "        scale_inplace(g_u, scale)\n",
    "        clipped_users.append(g_u)\n",
    "\n",
    "    U_B = len(uniq)\n",
    "    avg_cpu = zero_like_params(params, device=\"cpu\")\n",
    "    for g_u in clipped_users:\n",
    "        add_inplace(avg_cpu, g_u, alpha=1.0 / U_B)\n",
    "\n",
    "    std = noise_sigma * clip_C_user / U_B\n",
    "    noisy_avg_cpu = [g + torch.randn_like(g, device=\"cpu\") * std for g in avg_cpu]\n",
    "    private_grad = [g.to(model_device, non_blocking=True) for g in noisy_avg_cpu]\n",
    "    return private_grad, U_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dbba79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% updates: weight decay, DP-SGD, DP-DISFOM (prox on displacement, CPU-safe)\n",
    "def apply_weight_decay(params, wd):\n",
    "    if wd == 0: return\n",
    "    with torch.no_grad():\n",
    "        for p in params: p.add_(-wd * p)\n",
    "\n",
    "def step_dpsgd(model, lr, private_grad):\n",
    "    with torch.no_grad():\n",
    "        for p, g in zip(params_that_train(model), private_grad):\n",
    "            p.add_(-lr * g)\n",
    "\n",
    "# DISFOM prox φ(z)=(ρ̂/2)||z||_1^2 applied to displacement u = -lr·ĝ\n",
    "def tau_by_bisection(u: torch.Tensor, rho_hat: float, max_iter: int = 80, tol: float = 1e-12) -> float:\n",
    "    lo, hi = 0.0, float(u.abs().max().item())\n",
    "    def rhs(tau: float) -> float:\n",
    "        return float(torch.nn.functional.relu(u.abs() - tau).sum().item())\n",
    "    for _ in range(max_iter):\n",
    "        mid = 0.5*(lo+hi)\n",
    "        R = mid - rho_hat * rhs(mid)\n",
    "        if abs(R) <= tol: return mid\n",
    "        if R > 0: hi = mid\n",
    "        else:     lo = mid\n",
    "    return 0.5*(lo+hi)\n",
    "\n",
    "def step_dpdisfom(model, lr, private_grad, rho_hat, box_lo=None, box_hi=None):\n",
    "    \"\"\"\n",
    "    CPU-friendly prox: flatten on CPU to avoid peak GPU memory.\n",
    "    \"\"\"\n",
    "    params = params_that_train(model)\n",
    "    dev = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        # move to CPU\n",
    "        xk_list = [p.data.detach().cpu().clone() for p in params]\n",
    "        g_list  = [g.detach().cpu().clone() for g in private_grad]\n",
    "\n",
    "        xk_vec = torch.cat([t.view(-1) for t in xk_list])\n",
    "        g_vec  = torch.cat([t.view(-1) for t in g_list])\n",
    "        u = (-lr) * g_vec\n",
    "\n",
    "        if box_lo is None and box_hi is None:\n",
    "            tau = tau_by_bisection(u, rho_hat)\n",
    "            y = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "            x_next = xk_vec + y\n",
    "        else:\n",
    "            l = torch.full_like(xk_vec, box_lo) if box_lo is not None else torch.full_like(xk_vec, -float(\"inf\"))\n",
    "            ubox = torch.full_like(xk_vec, box_hi) if box_hi is not None else torch.full_like(xk_vec,  float(\"inf\"))\n",
    "            l_disp, u_disp = l - xk_vec, ubox - xk_vec\n",
    "            def rhs_box(tau: float) -> float:\n",
    "                z = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "                z = torch.max(torch.min(z, u_disp), l_disp)\n",
    "                return float(z.abs().sum().item())\n",
    "            lo, hi = 0.0, float(u.abs().max().item())\n",
    "            for _ in range(80):\n",
    "                mid = 0.5*(lo+hi)\n",
    "                R = mid - rho_hat * rhs_box(mid)\n",
    "                if abs(R) <= 1e-12: tau = mid; break\n",
    "                if R > 0: hi = mid\n",
    "                else:     lo = mid\n",
    "            else:\n",
    "                tau = 0.5*(lo+hi)\n",
    "            z = torch.sign(u) * torch.nn.functional.relu(u.abs() - tau)\n",
    "            y = torch.max(torch.min(z, u_disp), l_disp)\n",
    "            x_next = xk_vec + y\n",
    "\n",
    "        # write back to device params\n",
    "        offset = 0\n",
    "        for p in params:\n",
    "            n = p.numel()\n",
    "            chunk = x_next[offset:offset+n].view(p.shape).to(dev)\n",
    "            p.copy_(chunk)\n",
    "            offset += n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b78f786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% eval: PPL + ROUGE/BLEU\n",
    "def eval_perplexity(model, eval_loader):\n",
    "    model.eval(); total, count = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            for k in batch: batch[k] = batch[k].to(device)\n",
    "            out = model(**batch)\n",
    "            total += float(out.loss.item()) * batch[\"input_ids\"].size(0)\n",
    "            count += batch[\"input_ids\"].size(0)\n",
    "    mean = total / max(1, count)\n",
    "    ppl = math.exp(mean) if mean < 50 else float(\"inf\")\n",
    "    model.train()\n",
    "    return mean, ppl\n",
    "\n",
    "def split_prompt_and_ref(txt: str):\n",
    "    key = \"### Response:\\n\"\n",
    "    if key in txt:\n",
    "        i = txt.index(key)\n",
    "        return txt[:i+len(key)], txt[i+len(key):].strip()\n",
    "    return txt, \"\"\n",
    "\n",
    "def collect_eval_prompts_and_refs(dataset_validation, max_items=300):\n",
    "    prompts, refs = [], []\n",
    "    m = min(len(dataset_validation), max_items)\n",
    "    for i in range(m):\n",
    "        txt = dataset_validation[i][\"text\"]\n",
    "        p, r = split_prompt_and_ref(txt)\n",
    "        prompts.append(p); refs.append(r)\n",
    "    return prompts, refs\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_responses(model, tok, prompts, max_new_tokens=128, temperature=0.0, top_p=1.0):\n",
    "    outs=[]\n",
    "    for p in prompts:\n",
    "        inputs = tok(p, return_tensors=\"pt\").to(device)\n",
    "        ids = model.generate(**inputs, max_new_tokens=max_new_tokens,\n",
    "                             do_sample=(temperature>0), temperature=temperature, top_p=top_p,\n",
    "                             pad_token_id=tok.eos_token_id)\n",
    "        full = tok.decode(ids[0], skip_special_tokens=True)\n",
    "        outs.append(full[len(p):].strip() if full.startswith(p) else full.strip())\n",
    "    return outs\n",
    "\n",
    "def eval_text_metrics(preds, refs):\n",
    "    r = rouge.compute(predictions=preds, references=refs)\n",
    "    b = bleu.compute(predictions=preds, references=[[x] for x in refs])\n",
    "    return {**r, \"bleu\": b[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a28bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% privacy helper: σ for target ε (record-style search)\n",
    "def sigma_for_epsilon_record(target_eps, delta, q, T, lo=0.3, hi=5.0):\n",
    "    for _ in range(25):\n",
    "        mid = 0.5*(lo+hi)\n",
    "        acct = RDPAccountant()\n",
    "        for _ in range(T): acct.step(noise_multiplier=mid, sample_rate=q)\n",
    "        eps = acct.get_epsilon(delta=delta)\n",
    "        if eps > target_eps: lo = mid\n",
    "        else:                hi = mid\n",
    "    return hi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25eb341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% training loop (scenario-aware)\n",
    "def train_once(method: str, sigma: float, train_loader, eval_loader,\n",
    "               privacy_mode: str, total_users: Optional[int] = None) -> Dict[str, float]:\n",
    "    assert privacy_mode in (\"record\",\"user\")\n",
    "    model = build_model()\n",
    "    params = params_that_train(model)\n",
    "    lr = CFG.lr_disfom if method == \"dpdisfom\" else CFG.lr_sgd\n",
    "\n",
    "    acct = RDPAccountant()\n",
    "\n",
    "    run = None\n",
    "    if USE_WANDB:\n",
    "        run = wandb.init(\n",
    "            project=os.environ[\"WANDB_PROJECT\"],\n",
    "            entity=os.environ.get(\"WANDB_ENTITY\"),\n",
    "            name=f\"{method}-{privacy_mode}-sigma{sigma}-{int(time.time())}\",\n",
    "            group=f\"{CFG.scenario}\",\n",
    "            config={\n",
    "                \"base_model\": CFG.base_model, \"method\": method,\n",
    "                \"epochs\": CFG.epochs, \"clip\": CFG.clip_norm,\n",
    "                \"sigma\": sigma, \"delta\": CFG.delta,\n",
    "                \"rho_hat\": CFG.rho_hat if method==\"dpdisfom\" else None,\n",
    "                \"lr\": lr, \"batch\": CFG.micro_batch, \"seq_len\": CFG.max_seq_len,\n",
    "                \"privacy_mode\": privacy_mode, \"use_lora\": CFG.use_lora\n",
    "            },\n",
    "            reinit=True,\n",
    "        )\n",
    "        wandb.define_metric(\"train/iter\")\n",
    "        wandb.define_metric(\"train/loss_iter\", step_metric=\"train/iter\")\n",
    "        wandb.define_metric(\"epoch\")\n",
    "        wandb.define_metric(\"metrics/epsilon\", step_metric=\"epoch\")\n",
    "        wandb.define_metric(\"metrics/loss_epoch\", step_metric=\"epoch\")\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(CFG.epochs):\n",
    "        pbar = tqdm(train_loader, desc=f\"{method} [{privacy_mode}] σ={sigma} | epoch {ep+1}/{CFG.epochs}\")\n",
    "        for batch in pbar:\n",
    "            for k in batch: batch[k] = batch[k].to(device)\n",
    "\n",
    "            if privacy_mode == \"record\":\n",
    "                g_priv = dp_gradient_oracle_record(model, batch, CFG.clip_norm, sigma)\n",
    "                sample_rate = CFG.micro_batch / len(train_loader.dataset)\n",
    "            else:\n",
    "                g_priv, U_B = dp_gradient_oracle_user(model, batch, CFG.clip_norm, sigma)\n",
    "                if total_users is None or total_users == 0:\n",
    "                    raise ValueError(\"total_users must be provided for user-level DP.\")\n",
    "                sample_rate = U_B / total_users\n",
    "\n",
    "            # pre-update loss for logging (approx same step)\n",
    "            with torch.no_grad():\n",
    "                out = model(input_ids=batch[\"input_ids\"], labels=batch[\"labels\"], attention_mask=batch[\"attention_mask\"])\n",
    "                batch_loss = float(out.loss.detach().cpu().item())\n",
    "\n",
    "            apply_weight_decay(params, CFG.weight_decay)\n",
    "            if method == \"dpsgd\":\n",
    "                step_dpsgd(model, lr, g_priv)\n",
    "            elif method == \"dpdisfom\":\n",
    "                step_dpdisfom(model, lr, g_priv, CFG.rho_hat, CFG.box_lo, CFG.box_hi)\n",
    "            else:\n",
    "                raise ValueError(method)\n",
    "\n",
    "            acct.step(noise_multiplier=sigma, sample_rate=sample_rate)\n",
    "\n",
    "            global_step += 1\n",
    "            pbar.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "            if run is not None:\n",
    "                wandb.log({\"train/iter\": global_step, \"train/loss_iter\": batch_loss}, step=global_step)\n",
    "\n",
    "        eval_loss, eval_ppl = eval_perplexity(model, eval_loader)\n",
    "        eps_now = acct.get_epsilon(delta=CFG.delta)\n",
    "        if run is not None:\n",
    "            wandb.log({\"epoch\": ep+1,\n",
    "                       \"metrics/loss_epoch\": eval_loss,\n",
    "                       \"metrics/ppl\": eval_ppl,\n",
    "                       \"metrics/epsilon\": eps_now}, step=global_step)\n",
    "\n",
    "    # save\n",
    "    tag = f\"{CFG.base_model.replace('/','_')}_{method}_{privacy_mode}_sigma{sigma}\"\n",
    "    outdir = Path(CFG.save_root)/tag\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(outdir.as_posix())\n",
    "\n",
    "    # final eval\n",
    "    prompts, refs = collect_eval_prompts_and_refs(dataset[\"validation\"], max_items=300)\n",
    "    model.eval()\n",
    "    preds = generate_responses(model, tokenizer, prompts, max_new_tokens=128, temperature=0.0)\n",
    "    text_metrics = eval_text_metrics(preds, refs)\n",
    "    ppl_loss, ppl = eval_perplexity(model, eval_loader)\n",
    "\n",
    "    if run is not None:\n",
    "        wandb.log({\n",
    "            \"final/rouge1\": text_metrics[\"rouge1\"],\n",
    "            \"final/rouge2\": text_metrics[\"rouge2\"],\n",
    "            \"final/rougeL\": text_metrics[\"rougeL\"],\n",
    "            \"final/bleu\":   text_metrics[\"bleu\"],\n",
    "            \"final/ppl_loss\": ppl_loss,\n",
    "            \"final/ppl\": ppl\n",
    "        })\n",
    "        run.finish()\n",
    "\n",
    "    return {\"model_dir\": outdir.as_posix(),\n",
    "            \"rougeL\": text_metrics[\"rougeL\"],\n",
    "            \"bleu\": text_metrics[\"bleu\"],\n",
    "            \"ppl\": ppl, \"ppl_loss\": ppl_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98e059f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% orchestrators\n",
    "def run_matched_epsilon_record():\n",
    "    print(\"== Matched-ε (Record-level DP) ==\")\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    T = CFG.epochs * steps_per_epoch\n",
    "    q = CFG.micro_batch / len(train_loader.dataset)\n",
    "    sigma = sigma_for_epsilon_record(CFG.target_epsilon, CFG.delta, q, T, lo=0.3, hi=5.0)\n",
    "    print(f\"ε_target={CFG.target_epsilon} -> σ≈{sigma:.3f} (q={q:.6f}, T={T})\")\n",
    "\n",
    "    res = {}\n",
    "    for method in (\"dpsgd\", \"dpdisfom\"):\n",
    "    # for method in (\"dpdisfom\",):\n",
    "        res[method] = train_once(method, sigma, train_loader, eval_loader, privacy_mode=\"record\")\n",
    "        print(method, res[method])\n",
    "    return res, sigma\n",
    "\n",
    "def run_matched_epsilon_user():\n",
    "    assert TOTAL_USERS is not None, \"Rebuild dataset with add_user_ids=True.\"\n",
    "    print(\"== Matched-ε (User-level DP) ==\")\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    T = CFG.epochs * steps_per_epoch\n",
    "    # expected distinct users per batch ~ micro_batch (Poisson(5) gives many-user batches); conservative:\n",
    "    q_user = min(1.0, CFG.micro_batch / max(1, TOTAL_USERS))\n",
    "    sigma = sigma_for_epsilon_record(CFG.target_epsilon, CFG.delta, q_user, T, lo=0.3, hi=5.0)\n",
    "    print(f\"ε_target={CFG.target_epsilon} -> σ≈{sigma:.3f} (q_user≈{q_user:.6f}, T={T}, users={TOTAL_USERS})\")\n",
    "\n",
    "    res = {}\n",
    "    for method in (\"dpsgd\", \"dpdisfom\"):\n",
    "    # for method in (\"dpdisfom\",):\n",
    "        res[method] = train_once(method, sigma, train_loader, eval_loader, privacy_mode=\"user\", total_users=TOTAL_USERS)\n",
    "        print(method, res[method])\n",
    "    return res, sigma\n",
    "\n",
    "def run_noise_stress():\n",
    "    print(\"== Noise-Stress (σ sweep) ==\")\n",
    "    table = []\n",
    "    for sigma in CFG.stress_sigmas:\n",
    "        for method in (\"dpsgd\", \"dpdisfom\"):\n",
    "            res = train_once(method, sigma, train_loader, eval_loader, privacy_mode=\"record\")\n",
    "            print(f\"sigma={sigma:.2f}\", method, res)\n",
    "            table.append({\"sigma\": sigma, \"method\": method, **res})\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14215d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Matched-ε (Record-level DP) ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ε_target=1 -> σ≈1.005 (q=0.002000, T=5000)\n",
      "[Model] LoRA disabled (full fine-tune).\n",
      "[Model] Gradient checkpointing enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/2/bi000050/courses/stat8931/dp/dp_disfom/wandb/run-20251202_192039-bde1iboh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/bde1iboh' target=\"_blank\">dpsgd-record-sigma1.00489399433136-1764724839</a></strong> to <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/bde1iboh' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/bde1iboh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e303416638dd4efc987ccc4189500774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 1/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bd668b32004c88a92327ae9dcaca14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 2/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d476a7cccdd405fa9d69777d8f4a2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 3/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b74933fa0f9452dbaa129a0b8a3ea6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 4/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0aa466c93a048dbb00691a227cd6088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 5/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9612b961ee44dc89371d72ea5ec6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 6/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755489c4a2914c6baf0c246c50387ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 7/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1316434bf8415a84cbb57ef4890e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 8/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8491a0cba442bc9e5826047ca7a549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 9/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e90a760af94cad825d2f061a888b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpsgd [record] σ=1.00489399433136 | epoch 10/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final/bleu</td><td>▁</td></tr><tr><td>final/ppl</td><td>▁</td></tr><tr><td>final/ppl_loss</td><td>▁</td></tr><tr><td>final/rouge1</td><td>▁</td></tr><tr><td>final/rouge2</td><td>▁</td></tr><tr><td>final/rougeL</td><td>▁</td></tr><tr><td>metrics/epsilon</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>metrics/loss_epoch</td><td>▁▂▃▄▄▅▆▇▇█</td></tr><tr><td>metrics/ppl</td><td>▁▁▂▃▄▅▆▆▇█</td></tr><tr><td>train/iter</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇█</td></tr><tr><td>train/loss_iter</td><td>▃▄▂▃▄▇▁▂▂▁▆▅▆▃▅▅▄▇▂▃▂▆▆▅▅▅▆▃▆▅▅▅▅▇▇▆▆█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final/bleu</td><td>0.64537</td></tr><tr><td>final/ppl</td><td>31.23352</td></tr><tr><td>final/ppl_loss</td><td>3.44149</td></tr><tr><td>final/rouge1</td><td>0.02937</td></tr><tr><td>final/rouge2</td><td>0.00779</td></tr><tr><td>final/rougeL</td><td>0.02621</td></tr><tr><td>metrics/epsilon</td><td>1.0</td></tr><tr><td>metrics/loss_epoch</td><td>3.44149</td></tr><tr><td>metrics/ppl</td><td>31.23352</td></tr><tr><td>train/iter</td><td>5000</td></tr><tr><td>train/loss_iter</td><td>3.45045</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dpsgd-record-sigma1.00489399433136-1764724839</strong> at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/bde1iboh' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/bde1iboh</a><br> View project at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251202_192039-bde1iboh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpsgd {'model_dir': 'dp_llm_runs/gpt2_dpsgd_record_sigma1.00489399433136', 'rougeL': np.float64(0.026207653109886965), 'bleu': 0.6453729696983327, 'ppl': 31.23352314248056, 'ppl_loss': 3.4414919776916504}\n",
      "[Model] LoRA disabled (full fine-tune).\n",
      "[Model] Gradient checkpointing enabled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/2/bi000050/courses/stat8931/dp/dp_disfom/wandb/run-20251202_213159-a3u3gfae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/a3u3gfae' target=\"_blank\">dpdisfom-record-sigma1.00489399433136-1764732719</a></strong> to <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/a3u3gfae' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/a3u3gfae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7c45d090b145ddaa1248f42c2dbdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 1/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06251d2498c94f30bf5c6a210930c995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 2/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cae8983361d4d65887f967dd0c32b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 3/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334a30843ae4401dbb6a8634e85665b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 4/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d91bf314d434961aabac916908726ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 5/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d6a0ef889a47cca5eee3aac671f0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 6/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe38eed4a074051a5e3f0b5eae4c29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 7/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216d62c719e446f1b01f025258d5b3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 8/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f713c329907f4f7dad0c6af97d7da2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 9/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ba36d0249a42748e67d40df69064aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dpdisfom [record] σ=1.00489399433136 | epoch 10/10:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/2/bi000050/.conda/envs/opt/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final/bleu</td><td>▁</td></tr><tr><td>final/ppl</td><td>▁</td></tr><tr><td>final/ppl_loss</td><td>▁</td></tr><tr><td>final/rouge1</td><td>▁</td></tr><tr><td>final/rouge2</td><td>▁</td></tr><tr><td>final/rougeL</td><td>▁</td></tr><tr><td>metrics/epsilon</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>metrics/loss_epoch</td><td>██▁▁▃▂▃▂▄▄</td></tr><tr><td>metrics/ppl</td><td>██▁▁▃▂▃▂▄▄</td></tr><tr><td>train/iter</td><td>▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/loss_iter</td><td>▄▃▄▄▂▃▃▇▅▆▄▇▁▅▅▄█▃▂▄▃▅▃▃▃▃▃▃▄█▃▃▂▃▇▄▆▅▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>final/bleu</td><td>2.76757</td></tr><tr><td>final/ppl</td><td>20.11258</td></tr><tr><td>final/ppl_loss</td><td>3.00135</td></tr><tr><td>final/rouge1</td><td>0.14397</td></tr><tr><td>final/rouge2</td><td>0.04439</td></tr><tr><td>final/rougeL</td><td>0.12679</td></tr><tr><td>metrics/epsilon</td><td>1.0</td></tr><tr><td>metrics/loss_epoch</td><td>3.00135</td></tr><tr><td>metrics/ppl</td><td>20.11258</td></tr><tr><td>train/iter</td><td>5000</td></tr><tr><td>train/loss_iter</td><td>2.79472</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dpdisfom-record-sigma1.00489399433136-1764732719</strong> at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/a3u3gfae' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft/runs/a3u3gfae</a><br> View project at: <a href='https://wandb.ai/bi000050-university-of-minnesota/userdp_ft' target=\"_blank\">https://wandb.ai/bi000050-university-of-minnesota/userdp_ft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251202_213159-a3u3gfae/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpdisfom {'model_dir': 'dp_llm_runs/gpt2_dpdisfom_record_sigma1.00489399433136', 'rougeL': np.float64(0.12679352260761162), 'bleu': 2.7675663702625712, 'ppl': 20.11257684560192, 'ppl_loss': 3.001345333099365}\n"
     ]
    }
   ],
   "source": [
    "# %% execute chosen scenario\n",
    "if CFG.scenario == \"matched_epsilon_record\":\n",
    "    results, sigma_star = run_matched_epsilon_record()\n",
    "elif CFG.scenario == \"matched_epsilon_user\":\n",
    "    results, sigma_star = run_matched_epsilon_user()\n",
    "elif CFG.scenario == \"noise_stress\":\n",
    "    stress_table = run_noise_stress()\n",
    "else:\n",
    "    raise ValueError(\"Unknown CFG.scenario\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedfb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae85bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
